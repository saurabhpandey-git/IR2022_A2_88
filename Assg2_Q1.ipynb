{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4110f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fae14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Humor,Hist,Media,Food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad8c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de026a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1st_aid.txt',\n",
       " 'a-team',\n",
       " 'abbott.txt',\n",
       " 'aboutada.txt',\n",
       " 'acetab1.txt',\n",
       " 'aclamt.txt',\n",
       " 'acne1.txt',\n",
       " 'acronym.lis',\n",
       " 'acronym.txt',\n",
       " 'acronyms.txt',\n",
       " 'adameve.hum',\n",
       " 'adcopy.hum',\n",
       " 'addrmeri.txt',\n",
       " 'admin.txt',\n",
       " 'adrian_e.faq',\n",
       " 'ads.txt',\n",
       " 'adt_miam.txt',\n",
       " 'advrtize.txt',\n",
       " 'aeonint.txt',\n",
       " 'age.txt',\n",
       " 'aggie.txt',\n",
       " 'aids.txt',\n",
       " 'airlines',\n",
       " 'alabama.txt',\n",
       " 'alcatax.txt',\n",
       " 'alcohol.hum',\n",
       " 'alflog.txt',\n",
       " 'allfam.epi',\n",
       " 'allusion',\n",
       " 'all_grai',\n",
       " 'amazing.epi',\n",
       " 'ambrose.bie',\n",
       " 'amchap2.txt',\n",
       " 'analogy.hum',\n",
       " 'aniherb.txt',\n",
       " 'anime.cli',\n",
       " 'anime.lif',\n",
       " 'anim_lif.txt',\n",
       " 'annoy.fascist',\n",
       " 'anorexia.txt',\n",
       " 'answers',\n",
       " 'anthropo.stu',\n",
       " 'antibiot.txt',\n",
       " 'antimead.bev',\n",
       " 'aphrodis.txt',\n",
       " 'appbred.brd',\n",
       " 'appetiz.rcp',\n",
       " 'applepie.des',\n",
       " 'apsaucke.des',\n",
       " 'apsnet.txt',\n",
       " 'arab.dic',\n",
       " 'arcadian.txt',\n",
       " 'argotdic.txt',\n",
       " 'arnold.txt',\n",
       " 'art-fart.hum',\n",
       " 'arthriti.txt',\n",
       " 'ateam.epi',\n",
       " 'atherosc.txt',\n",
       " 'atombomb.hum',\n",
       " 'att.txt',\n",
       " 'aussie.lng',\n",
       " 'avengers.lis',\n",
       " 'awespinh.sal',\n",
       " 'ayurved.txt',\n",
       " 'a_fish_c.apo',\n",
       " 'a_tv_t-p.com',\n",
       " 'b-2.jok',\n",
       " 'b12.txt',\n",
       " 'back1.txt',\n",
       " 'bad',\n",
       " 'bad-d',\n",
       " 'bad.jok',\n",
       " 'badday.hum',\n",
       " 'bagelope.txt',\n",
       " 'bakebred.txt',\n",
       " 'baklava.des',\n",
       " 'banana01.brd',\n",
       " 'banana02.brd',\n",
       " 'banana03.brd',\n",
       " 'banana04.brd',\n",
       " 'banana05.brd',\n",
       " 'bank.rob',\n",
       " 'barney.cn1',\n",
       " 'barney.txt',\n",
       " 'basehead.txt',\n",
       " 'batrbred.txt',\n",
       " 'bb',\n",
       " 'bbc_vide.cat',\n",
       " 'bbh_intv.txt',\n",
       " 'bbq.txt',\n",
       " 'beapimp.hum',\n",
       " 'beauty.tm',\n",
       " 'beave.hum',\n",
       " 'beer-g',\n",
       " 'beer-gui',\n",
       " 'beer.gam',\n",
       " 'beer.hum',\n",
       " 'beer.txt',\n",
       " 'beerdiag.txt',\n",
       " 'beergame.hum',\n",
       " 'beergame.txt',\n",
       " 'beerjesus.hum',\n",
       " 'beershrm.fis',\n",
       " 'beershrp.fis',\n",
       " 'beerwarn.txt',\n",
       " 'beesherb.txt',\n",
       " 'beginn.ers',\n",
       " 'berryeto.bev',\n",
       " 'bhang.fun',\n",
       " 'bhb.ill',\n",
       " 'bible.txt',\n",
       " 'bigpic1.hum',\n",
       " 'billcat.hum',\n",
       " 'bimg.prn',\n",
       " 'bingbong.hum',\n",
       " 'bitchcar.hum',\n",
       " 'bitnet.txt',\n",
       " 'blackadd',\n",
       " 'blackapp.hum',\n",
       " 'blackhol.hum',\n",
       " 'blake7.lis',\n",
       " 'blaster.hum',\n",
       " 'bless.bc',\n",
       " 'blkbean.txt',\n",
       " 'blkbnsrc.vgn',\n",
       " 'blood.txt',\n",
       " 'blooprs1.asc',\n",
       " 'bmdn01.txt',\n",
       " 'bnbeg2.4.txt',\n",
       " 'bnbguide.txt',\n",
       " 'bnb_quot.txt',\n",
       " 'boarchil.txt',\n",
       " 'boatmemo.jok',\n",
       " 'boe.hum',\n",
       " 'bond-2.txt',\n",
       " 'boneles2.txt',\n",
       " 'booknuti.txt',\n",
       " 'booze.fun',\n",
       " 'booze1.fun',\n",
       " 'booze2.fun',\n",
       " 'bored.txt',\n",
       " 'boston.geog',\n",
       " 'bozo_tv.leg',\n",
       " 'brainect.hum',\n",
       " 'brdpudd.des',\n",
       " 'bread.rcp',\n",
       " 'bread.rec',\n",
       " 'bread.txt',\n",
       " 'breadpud.des',\n",
       " 'bredcake.des',\n",
       " 'brewing',\n",
       " 'browneco.hum',\n",
       " 'brownie.rec',\n",
       " 'brush1.txt',\n",
       " 'btaco.txt',\n",
       " 'btcisfre.hum',\n",
       " 'btscke01.des',\n",
       " 'btscke02.des',\n",
       " 'btscke03.des',\n",
       " 'btscke04.des',\n",
       " 'btscke05.des',\n",
       " 'buffwing.pol',\n",
       " 'bugbreak.hum',\n",
       " 'bugs.txt',\n",
       " 'buldrwho.txt',\n",
       " 'bunacald.fis',\n",
       " 'burrito.mea',\n",
       " 'butcher.txt',\n",
       " 'butstcod.fis',\n",
       " 'butwrong.hum',\n",
       " 'buzzword.hum',\n",
       " 'bw-phwan.hat',\n",
       " 'bw-summe.hat',\n",
       " 'bw.txt',\n",
       " 'byfb.txt',\n",
       " 'c0dez.txt',\n",
       " 'cabbage.txt',\n",
       " 'caesardr.sal',\n",
       " 'cake.rec',\n",
       " 'calamus.hrb',\n",
       " 'calculus.txt',\n",
       " 'calif.hum',\n",
       " 'calvin.txt',\n",
       " 'cancer.rat',\n",
       " 'candy.txt',\n",
       " 'candybar.fun',\n",
       " 'capital.txt',\n",
       " 'caramels.des',\n",
       " 'carowner.txt',\n",
       " 'cars.txt',\n",
       " 'cartoon.law',\n",
       " 'cartoon.laws',\n",
       " 'cartoon_.txt',\n",
       " 'cartoon_laws.txt',\n",
       " 'cartwb.son',\n",
       " 'cast.lis',\n",
       " 'catballs.hum',\n",
       " 'catin.hat',\n",
       " 'catranch.hum',\n",
       " 'catstory.txt',\n",
       " 'cbmatic.hum',\n",
       " 'cereal.txt',\n",
       " 'cform2.txt',\n",
       " 'cgs_lst.txt',\n",
       " 'chainltr.txt',\n",
       " 'change.hum',\n",
       " 'charity.hum',\n",
       " 'cheapfar.hum',\n",
       " 'cheapin.la',\n",
       " 'chickenheadbbs.txt',\n",
       " 'chickens.jok',\n",
       " 'chickens.txt',\n",
       " 'childhoo.jok',\n",
       " 'childrenbooks.txt',\n",
       " 'chili.txt',\n",
       " 'chinese.txt',\n",
       " 'chinesec.hum',\n",
       " 'choco-ch.ips',\n",
       " 'christop.int',\n",
       " 'chung.iv',\n",
       " 'chunnel.txt',\n",
       " 'church.sto',\n",
       " 'clancy.txt',\n",
       " 'classicm.hum',\n",
       " 'climbing.let',\n",
       " 'cmu.share',\n",
       " 'co-car.jok',\n",
       " 'cockney.alp',\n",
       " 'coffee.faq',\n",
       " 'coffee.txt',\n",
       " 'coffeebeerwomen.txt',\n",
       " 'cogdis.txt',\n",
       " 'coke.fun',\n",
       " 'coke.txt',\n",
       " 'coke1',\n",
       " 'cokeform.txt',\n",
       " 'coke_fan.naz',\n",
       " 'coladrik.fun',\n",
       " 'coladrik.txt',\n",
       " 'cold.fus',\n",
       " 'coldfake.hum',\n",
       " 'collected_quotes.txt',\n",
       " 'college.hum',\n",
       " 'college.sla',\n",
       " 'college.txt',\n",
       " 'comic_st.gui',\n",
       " 'commutin.jok',\n",
       " 'commword.hum',\n",
       " 'computer.txt',\n",
       " 'comrevi1.hum',\n",
       " 'conan.txt',\n",
       " 'confucius_say.txt',\n",
       " 'consp.txt',\n",
       " 'contract.moo',\n",
       " 'cookberk',\n",
       " 'cookbkly.how',\n",
       " 'cookie.1',\n",
       " 'cooking.fun',\n",
       " 'cooking.jok',\n",
       " 'coollngo2.txt',\n",
       " 'cooplaws',\n",
       " 'cops.txt',\n",
       " 'corporat.txt',\n",
       " 'court.quips',\n",
       " 'cowexplo.hum',\n",
       " 'coyote.txt',\n",
       " 'crazy.txt',\n",
       " 'critic.txt',\n",
       " 'crzycred.lst',\n",
       " 'cuchy.hum',\n",
       " 'cucumber.jok',\n",
       " 'cucumber.txt',\n",
       " 'cuisine.txt',\n",
       " 'cultmov.faq',\n",
       " 'curiousgeorgie.txt',\n",
       " 'curry.hrb',\n",
       " 'curry.txt',\n",
       " 'curse.txt',\n",
       " 'cybrtrsh.txt',\n",
       " 'd-ned.hum',\n",
       " 'dalive',\n",
       " 'damiana.hrb',\n",
       " 'dandwine.bev',\n",
       " 'dark.suc',\n",
       " 'dead-r',\n",
       " 'dead2.txt',\n",
       " 'dead3.txt',\n",
       " 'dead4.txt',\n",
       " 'dead5.txt',\n",
       " 'deadlysins.txt',\n",
       " 'deathhem.txt',\n",
       " 'deep.txt',\n",
       " 'defectiv.hum',\n",
       " 'desk.txt',\n",
       " 'deterior.hum',\n",
       " 'devils.jok',\n",
       " 'diesmurf.txt',\n",
       " 'diet.txt',\n",
       " 'dieter.txt',\n",
       " 'dingding.hum',\n",
       " 'dining.out',\n",
       " 'dirtword.txt',\n",
       " 'disaster.hum',\n",
       " 'disclmr.txt',\n",
       " 'disclym.txt',\n",
       " 'doc-says.txt',\n",
       " 'docdict.txt',\n",
       " 'docspeak.txt',\n",
       " 'doggun.sto',\n",
       " 'donut.txt',\n",
       " 'dover.poem',\n",
       " 'draxamus.txt',\n",
       " 'drinker.txt',\n",
       " 'drinking.tro',\n",
       " 'drinkrul.jok',\n",
       " 'drinks.gui',\n",
       " 'drinks.txt',\n",
       " 'drive.txt',\n",
       " 'dromes.txt',\n",
       " 'druggame.hum',\n",
       " 'drugshum.hum',\n",
       " 'drunk.txt',\n",
       " 'dthought.txt',\n",
       " 'dubltalk.jok',\n",
       " 'dym',\n",
       " 'eandb.drx',\n",
       " 'earp',\n",
       " 'eatme.txt',\n",
       " 'econridl.fun',\n",
       " 'egg-bred.txt',\n",
       " 'egglentl.vgn',\n",
       " 'eggroll1.mea',\n",
       " 'electric.txt',\n",
       " 'element.jok',\n",
       " 'elephant.fun',\n",
       " 'elevator.fun',\n",
       " 'empeval.txt',\n",
       " 'engineer.hum',\n",
       " 'english',\n",
       " 'english.txt',\n",
       " 'engmuffn.txt',\n",
       " 'engrhyme.txt',\n",
       " 'enlightenment.txt',\n",
       " 'enquire.hum',\n",
       " 'epikarat.txt',\n",
       " 'epiquest.txt',\n",
       " 'episimp2.txt',\n",
       " 'epitaph',\n",
       " 'epi_.txt',\n",
       " 'epi_bnb.txt',\n",
       " 'epi_merm.txt',\n",
       " 'epi_rns.txt',\n",
       " 'epi_tton.txt',\n",
       " 'eskimo.nel',\n",
       " 'exam.50',\n",
       " 'excuse.txt',\n",
       " 'excuse30.txt',\n",
       " 'excuses.txt',\n",
       " 'exidy.txt',\n",
       " 'exylic.txt',\n",
       " 'facedeth.txt',\n",
       " 'failure.txt',\n",
       " 'fajitas.rcp',\n",
       " 'farsi.phrase',\n",
       " 'farsi.txt',\n",
       " 'fartinfo.txt',\n",
       " 'fartting.txt',\n",
       " 'fascist.txt',\n",
       " 'fbipizza.txt',\n",
       " 'fearcola.hum',\n",
       " 'fed.txt',\n",
       " 'fegg!int.txt',\n",
       " 'feggaqui.txt',\n",
       " 'feggmagi.txt',\n",
       " 'feista01.dip',\n",
       " 'female.jok',\n",
       " 'fiber.txt',\n",
       " 'figure_1.txt',\n",
       " 'filmgoof.txt',\n",
       " 'films_gl.txt',\n",
       " 'final-ex.txt',\n",
       " 'finalexm.hum',\n",
       " 'firecamp.txt',\n",
       " 'fireplacein.txt',\n",
       " 'firstaid.inf',\n",
       " 'firstaid.txt',\n",
       " 'fish.rec',\n",
       " 'flattax.hum',\n",
       " 'flowchrt',\n",
       " 'flowchrt.txt',\n",
       " 'flux_fix.txt',\n",
       " 'focaccia.brd',\n",
       " 'food',\n",
       " 'foodtips',\n",
       " 'footfun.hum',\n",
       " 'forsooth.hum',\n",
       " 'free-cof.fee',\n",
       " 'freshman.hum',\n",
       " 'freudonseuss.txt',\n",
       " 'frogeye1.sal',\n",
       " 'from.hum',\n",
       " 'fuck!.txt',\n",
       " 'fuckyou2.txt',\n",
       " 'fudge.txt',\n",
       " 'fusion.gal',\n",
       " 'fusion.sup',\n",
       " 'fwksfun.hum',\n",
       " 'f_tang.txt',\n",
       " 'gack!.txt',\n",
       " 'gaiahuma',\n",
       " 'gameshow.txt',\n",
       " 'ganamembers.txt',\n",
       " 'garlpast.vgn',\n",
       " 'gas.txt',\n",
       " 'gd_alf.txt',\n",
       " 'gd_drwho.txt',\n",
       " 'gd_flybd.txt',\n",
       " 'gd_frasr.txt',\n",
       " 'gd_gal.txt',\n",
       " 'gd_guide.txt',\n",
       " 'gd_hhead.txt',\n",
       " 'gd_liqtv.txt',\n",
       " 'gd_maxhd.txt',\n",
       " 'gd_ol.txt',\n",
       " 'gd_ql.txt',\n",
       " 'gd_sgrnd.txt',\n",
       " 'gd_tznew.txt',\n",
       " 'german.aut',\n",
       " 'get.drunk.cheap',\n",
       " 'ghostfun.hum',\n",
       " 'ghostsch.hum',\n",
       " 'gingbeer.txt',\n",
       " 'girlspeak.txt',\n",
       " 'godmonth.txt',\n",
       " 'goforth.hum',\n",
       " 'gohome.hum',\n",
       " 'goldwatr.txt',\n",
       " 'golnar.txt',\n",
       " 'good.txt',\n",
       " 'gotukola.hrb',\n",
       " 'gown.txt',\n",
       " 'grail.txt',\n",
       " 'grammar.jok',\n",
       " 'greenchi.txt',\n",
       " 'grommet.hum',\n",
       " 'grospoem.txt',\n",
       " 'growth.txt',\n",
       " 'gumbo.txt',\n",
       " 'hack',\n",
       " 'hack7.txt',\n",
       " 'hackingcracking.txt',\n",
       " 'hackmorality.txt',\n",
       " 'hacktest.txt',\n",
       " 'hamburge.nam',\n",
       " 'hammock.hum',\n",
       " 'hangover.txt',\n",
       " 'happyhack.txt',\n",
       " 'harmful.hum',\n",
       " 'hate.hum',\n",
       " 'hbo_spec.rev',\n",
       " 'headlnrs',\n",
       " 'hecomes.jok',\n",
       " 'hedgehog.txt',\n",
       " 'height.txt',\n",
       " 'hell.jok',\n",
       " 'hell.txt',\n",
       " 'herb!.hum',\n",
       " 'hermsys.txt',\n",
       " 'heroic.txt',\n",
       " 'hi.tec',\n",
       " 'hierarch.txt',\n",
       " 'highland.epi',\n",
       " 'hilbilly.wri',\n",
       " 'history2.oop',\n",
       " 'hitchcoc.app',\n",
       " 'hitchcok.txt',\n",
       " 'hitler.59',\n",
       " 'hitler.txt',\n",
       " 'hitlerap.txt',\n",
       " 'homebrew.txt',\n",
       " 'homermmm.txt',\n",
       " 'hoonsrc.txt',\n",
       " 'hoosier.txt',\n",
       " 'hop.faq',\n",
       " 'horflick.txt',\n",
       " 'horoscop.jok',\n",
       " 'horoscop.txt',\n",
       " 'horoscope.txt',\n",
       " 'hotel.txt',\n",
       " 'hotnnot.hum',\n",
       " 'hotpeper.txt',\n",
       " 'how.bugs.breakd',\n",
       " 'how2bgod.txt',\n",
       " 'how2dotv.txt',\n",
       " 'howlong.hum',\n",
       " 'how_to_i.pro',\n",
       " 'hstlrtxt.txt',\n",
       " 'htswfren.txt',\n",
       " 'hum2',\n",
       " 'humatra.txt',\n",
       " 'humatran.jok',\n",
       " 'humor9.txt',\n",
       " 'humpty.dumpty',\n",
       " 'iced.tea',\n",
       " 'icm.hum',\n",
       " 'idaho.txt',\n",
       " 'idr2.txt',\n",
       " 'imbecile.txt',\n",
       " 'imprrisk.hum',\n",
       " 'impurmat.hum',\n",
       " 'incarhel.hum',\n",
       " 'indgrdn.txt',\n",
       " 'initials.rid',\n",
       " 'inlaws1.txt',\n",
       " 'inquirer.txt',\n",
       " 'ins1',\n",
       " 'insanity.hum',\n",
       " 'insect1.txt',\n",
       " 'insult',\n",
       " 'insult.lst',\n",
       " 'insults1.txt',\n",
       " 'insuranc.sty',\n",
       " 'insure.hum',\n",
       " 'interv.hum',\n",
       " 'investi.hum',\n",
       " 'iqtest',\n",
       " 'iremember',\n",
       " 'is_story.txt',\n",
       " 'italoink.txt',\n",
       " 'ivan.hum',\n",
       " 'jac&tuu.hum',\n",
       " 'jalapast.dip',\n",
       " 'jambalay.pol',\n",
       " 'japantv.txt',\n",
       " 'japice.bev',\n",
       " 'japrap.hum',\n",
       " 'jargon.phd',\n",
       " 'jason.fun',\n",
       " 'jawgumbo.fis',\n",
       " 'jawsalad.fis',\n",
       " 'jayjay.txt',\n",
       " 'jc-elvis.inf',\n",
       " 'jeffie.heh',\n",
       " 'jerky.rcp',\n",
       " 'jimhood.txt',\n",
       " 'johann',\n",
       " 'jokeju07.txt',\n",
       " 'jokes',\n",
       " 'jokes.txt',\n",
       " 'jokes1.txt',\n",
       " 'jon.txt',\n",
       " 'jrrt.riddle',\n",
       " 'jungjuic.bev',\n",
       " 'just2',\n",
       " 'justify',\n",
       " 'kaboom.hum',\n",
       " 'kanalx.txt',\n",
       " 'kashrut.txt',\n",
       " 'kid2',\n",
       " 'kid_diet.txt',\n",
       " 'killer.hum',\n",
       " 'killself.hum',\n",
       " 'kilroy',\n",
       " 'kilsmur.hum',\n",
       " 'kloo.txt',\n",
       " 'koans.txt',\n",
       " 'labels.txt',\n",
       " 'lampoon.jok',\n",
       " 'languag.jok',\n",
       " 'lansing.txt',\n",
       " 'law.sch',\n",
       " 'lawhunt.txt',\n",
       " 'laws.txt',\n",
       " 'lawskool.txt',\n",
       " 'lawsuniv.hum',\n",
       " 'lawyer.jok',\n",
       " 'lawyers.txt',\n",
       " 'lazarus.txt',\n",
       " 'la_times.hun',\n",
       " 'lbinter.hum',\n",
       " 'leech.txt',\n",
       " 'legal.hum',\n",
       " 'let.go',\n",
       " 'letgosh.txt',\n",
       " 'letter.txt',\n",
       " 'letterbx.txt',\n",
       " 'letter_f.sch',\n",
       " 'libraway.txt',\n",
       " 'liceprof.sty',\n",
       " 'lif&love.hum',\n",
       " 'lifeimag.hum',\n",
       " 'lifeinfo.hum',\n",
       " 'lifeonledge.txt',\n",
       " 'limerick.jok',\n",
       " 'lines.jok',\n",
       " 'lion.jok',\n",
       " 'lion.txt',\n",
       " 'lions.cat',\n",
       " 'lipkovits.txt',\n",
       " 'livnware.hum',\n",
       " 'llamas.txt',\n",
       " 'lll.hum',\n",
       " 'llong.hum',\n",
       " 'lobquad.hum',\n",
       " 'looser.hum',\n",
       " 'losers84.hum',\n",
       " 'losers86.hum',\n",
       " 'lost.txt',\n",
       " 'lotsa.jok',\n",
       " 'lozers',\n",
       " 'lozerzon.hum',\n",
       " 'lozeuser.hum',\n",
       " 'lp-assoc.txt',\n",
       " 'lucky.cha',\n",
       " 'ludeinfo.hum',\n",
       " 'ludeinfo.txt',\n",
       " 'luggage.hum',\n",
       " 'luvstory.txt',\n",
       " 'luzerzo2.hum',\n",
       " 'm0dzmen.hum',\n",
       " 'macsfarm.old',\n",
       " 'madhattr.jok',\n",
       " 'madscrib.hum',\n",
       " 'maecenas.hum',\n",
       " 'mailfrag.hum',\n",
       " 'makebeer.hum',\n",
       " 'making_y.wel',\n",
       " 'malechem.txt',\n",
       " 'manager.txt',\n",
       " 'manilla.hum',\n",
       " 'manners.txt',\n",
       " 'manspace.hum',\n",
       " 'margos.txt',\n",
       " 'marines.hum',\n",
       " 'marriage.hum',\n",
       " 'mash.hum',\n",
       " 'math.1',\n",
       " 'math.2',\n",
       " 'math.far',\n",
       " 'maxheadr',\n",
       " 'mcd.txt',\n",
       " 'mead.rcp',\n",
       " 'meat2.txt',\n",
       " 'meinkamp.hum',\n",
       " 'mel.txt',\n",
       " 'melodram.hum',\n",
       " 'memo.hum',\n",
       " 'memory.hum',\n",
       " 'men&wome.txt',\n",
       " 'mensroom.jok',\n",
       " 'merry.txt',\n",
       " 'miamadvi.hum',\n",
       " 'miami.hum',\n",
       " 'miamimth.txt',\n",
       " 'middle.age',\n",
       " 'mindvox',\n",
       " 'minn.txt',\n",
       " 'miranda.hum',\n",
       " 'misc.1',\n",
       " 'misery.hum',\n",
       " 'missdish',\n",
       " 'missheav.hum',\n",
       " 'mitch.txt',\n",
       " 'mlverb.hum',\n",
       " 'modemwld.txt',\n",
       " 'modest.hum',\n",
       " 'modstup',\n",
       " 'mog-history',\n",
       " 'montoys.txt',\n",
       " 'montpyth.hum',\n",
       " 'moonshin',\n",
       " 'moore.txt',\n",
       " 'moose.txt',\n",
       " 'moslem.txt',\n",
       " 'mothers.txt',\n",
       " 'motrbike.jok',\n",
       " 'mov_rail.txt',\n",
       " 'mowers.txt',\n",
       " 'mr.rogers',\n",
       " 'mrscienc.hum',\n",
       " 'mrsfield',\n",
       " 'msfields.txt',\n",
       " 'msorrow',\n",
       " 'mtm.hum',\n",
       " 'mtv.asc',\n",
       " 'mundane.v2',\n",
       " 'murph.jok',\n",
       " 'murphy.txt',\n",
       " 'murphys.txt',\n",
       " 'murphy_l.txt',\n",
       " 'mutate.hum',\n",
       " 'mydaywss.hum',\n",
       " 'myheart.hum',\n",
       " 'naivewiz.hum',\n",
       " 'namaste.txt',\n",
       " 'nameisreo.txt',\n",
       " 'namm',\n",
       " 'nasaglenn.txt',\n",
       " 'necropls.txt',\n",
       " 'netmask.txt',\n",
       " 'netnews.10',\n",
       " 'newcoke.txt',\n",
       " 'newconst.hum',\n",
       " 'newmex.hum',\n",
       " 'news.hum',\n",
       " 'nigel.1',\n",
       " 'nigel.10',\n",
       " 'nigel.2',\n",
       " 'nigel.3',\n",
       " 'nigel.4',\n",
       " 'nigel.5',\n",
       " 'nigel.6',\n",
       " 'nigel.7',\n",
       " 'nigel10.txt',\n",
       " 'nihgel_8.9',\n",
       " 'nintendo.jok',\n",
       " 'normal.boy',\n",
       " 'normalboy.txt',\n",
       " 'normquot.txt',\n",
       " 'nosuch_nasfic',\n",
       " 'novel.hum',\n",
       " 'nuke.hum',\n",
       " 'nukeplay.hum',\n",
       " 'nukewar.jok',\n",
       " 'nukewar.txt',\n",
       " 'nukwaste',\n",
       " 'number',\n",
       " 'number.killer',\n",
       " 'number_k.ill',\n",
       " 'nurds.hum',\n",
       " 'nysucks.hum',\n",
       " 'nzdrinks.txt',\n",
       " 'o-ttalk.hum',\n",
       " 'oakwood.txt',\n",
       " 'oam-001.txt',\n",
       " 'oam.nfo',\n",
       " 'oasis',\n",
       " 'oatbran.rec',\n",
       " 'oculis.rcp',\n",
       " 'odd_to.obs',\n",
       " 'odearakk.hum',\n",
       " 'office.txt',\n",
       " 'ohandre.hum',\n",
       " 'oilgluts.hum',\n",
       " 'old.txt',\n",
       " 'oldeng.hum',\n",
       " 'oldtime.sng',\n",
       " 'oldtime.txt',\n",
       " 'oliver.txt',\n",
       " 'oliver02.txt',\n",
       " 'onan.txt',\n",
       " 'one.par',\n",
       " 'onetoone.hum',\n",
       " 'onetotwo.hum',\n",
       " 'ookpik.hum',\n",
       " 'opinion.hum',\n",
       " 'oracle.jok',\n",
       " 'oranchic.pol',\n",
       " 'orgfrost.bev',\n",
       " 'ourfathr.txt',\n",
       " 'outawork.erl',\n",
       " 'outlimit.txt',\n",
       " 'oxymoron.jok',\n",
       " 'oxymoron.txt',\n",
       " 'ozarks.hum',\n",
       " 'p-law.hum',\n",
       " 'packard.txt',\n",
       " 'paddingurpapers.txt',\n",
       " 'parabl.hum',\n",
       " 'parades.hum',\n",
       " 'parsnip.txt',\n",
       " 'passage.hum',\n",
       " 'passenge.sim',\n",
       " 'pasta001.sal',\n",
       " 'pat.txt',\n",
       " 'pbcookie.des',\n",
       " 'peanuts.txt',\n",
       " 'peatchp.hum',\n",
       " 'pecker.txt',\n",
       " 'penisprt.txt',\n",
       " 'penndtch',\n",
       " 'pepper.txt',\n",
       " 'pepsideg.txt',\n",
       " 'petshop',\n",
       " 'phony.hum',\n",
       " 'phorse.hum',\n",
       " 'phunatdi.ana',\n",
       " 'phxbbs-m.txt',\n",
       " 'pickup.lin',\n",
       " 'pickup.txt',\n",
       " 'pipespec.txt',\n",
       " 'pizzawho.hum',\n",
       " 'planeget.hum',\n",
       " 'planetzero.txt',\n",
       " 'poets.hum',\n",
       " 'pol-corr.txt',\n",
       " 'polemom.txt',\n",
       " 'poli.tics',\n",
       " 'policpig.hum',\n",
       " 'poli_t.ics',\n",
       " 'poll2res.hum',\n",
       " 'polly.txt',\n",
       " 'polly_.new',\n",
       " 'poopie.txt',\n",
       " 'popconc.hum',\n",
       " 'popmach',\n",
       " 'popmusi.hum',\n",
       " 'post.nuc',\n",
       " 'pot.txt',\n",
       " 'potty.txt',\n",
       " 'pournell.spo',\n",
       " 'ppbeer.txt',\n",
       " 'prac1.jok',\n",
       " 'prac2.jok',\n",
       " 'prac3.jok',\n",
       " 'prac4.jok',\n",
       " 'pracjoke.txt',\n",
       " 'practica.txt',\n",
       " 'prawblim.hum',\n",
       " 'prayer.hum',\n",
       " 'primes.jok',\n",
       " 'princess.brd',\n",
       " 'pro-fact.hum',\n",
       " 'problem.txt',\n",
       " 'progrs.gph',\n",
       " 'proof.met',\n",
       " 'prooftec.txt',\n",
       " 'proposal.jok',\n",
       " 'proudlyserve.txt',\n",
       " 'prover.wisom',\n",
       " 'prover_w.iso',\n",
       " 'psalm.reagan',\n",
       " 'psalm23.txt',\n",
       " 'psalm_nixon',\n",
       " 'psalm_re.aga',\n",
       " 'psilaine.hum',\n",
       " 'psycho.txt',\n",
       " 'psych_pr.quo',\n",
       " 'pukeprom.jok',\n",
       " 'pun.txt',\n",
       " 'pure.mat',\n",
       " 'puzzle.spo',\n",
       " 'puzzles.jok',\n",
       " 'python_s.ong',\n",
       " 'q.pun',\n",
       " 'qttofu.vgn',\n",
       " 'quack26.txt',\n",
       " 'quantity.001',\n",
       " 'quantum.jok',\n",
       " 'quantum.phy',\n",
       " 'quest.hum',\n",
       " 'quick.jok',\n",
       " 'quotes.bug',\n",
       " 'quotes.jok',\n",
       " 'quotes.txt',\n",
       " 'quux_p.oem',\n",
       " 'rabbit.txt',\n",
       " 'racist.net',\n",
       " 'radexposed.txt',\n",
       " 'radiolaf.hum',\n",
       " 'rapmastr.hum',\n",
       " 'ratings.hum',\n",
       " 'ratspit.hum',\n",
       " 'raven.hum',\n",
       " 'readme.bat',\n",
       " 'reagan.hum',\n",
       " 'realest.txt',\n",
       " 'reasons.txt',\n",
       " 'rec.por',\n",
       " 'recepies.fun',\n",
       " 'recip1.txt',\n",
       " 'recipe.001',\n",
       " 'recipe.002',\n",
       " 'recipe.003',\n",
       " 'recipe.004',\n",
       " 'recipe.005',\n",
       " 'recipe.006',\n",
       " 'recipe.007',\n",
       " 'recipe.008',\n",
       " 'recipe.009',\n",
       " 'recipe.010',\n",
       " 'recipe.011',\n",
       " 'recipe.012',\n",
       " 'reconcil.hum',\n",
       " 'record_.gap',\n",
       " 'red-neck.jks',\n",
       " 'reddwarf.sng',\n",
       " 'reddye.hum',\n",
       " 'rednecks.txt',\n",
       " 'reeves.txt',\n",
       " 'relative.ada',\n",
       " 'religion.txt',\n",
       " 'renored.txt',\n",
       " 'renorthr.txt',\n",
       " 'rent-a_cat',\n",
       " 'rentals.hum',\n",
       " 'repair.hum',\n",
       " 'report.hum',\n",
       " 'research.hum',\n",
       " 'residncy.jok',\n",
       " 'resolutn.txt',\n",
       " 'resrch_p.hra',\n",
       " 'resrch_phrase',\n",
       " 'revolt.dj',\n",
       " 'richbred.txt',\n",
       " 'rinaldo.jok',\n",
       " 'rinaldos.law',\n",
       " 'rinaldos.txt',\n",
       " 'ripoffpc.hum',\n",
       " 'rns_bcl.txt',\n",
       " 'rns_bwl.txt',\n",
       " 'rns_ency.txt',\n",
       " 'roach.asc',\n",
       " 'roadpizz.txt',\n",
       " 'robot.tes',\n",
       " 'rocking.hum',\n",
       " 'rockmus.hum',\n",
       " 'sanshop.txt',\n",
       " 'saveface.hum',\n",
       " 'sawyer.txt',\n",
       " 'scam.txt',\n",
       " 'scratchy.txt',\n",
       " 'seafood.txt',\n",
       " 'seeds42.txt',\n",
       " 'sf-zine.pub',\n",
       " 'sfmovie.txt',\n",
       " 'shameonu.hum',\n",
       " 'shooters.txt',\n",
       " 'shorties.jok',\n",
       " 'shrink.news',\n",
       " 'shuimai.txt',\n",
       " 'shuttleb.hum',\n",
       " 'signatur.jok',\n",
       " 'sigs.txt',\n",
       " 'silverclaws.txt',\n",
       " 'simp.txt',\n",
       " 'sinksub.txt',\n",
       " 'skincat',\n",
       " 'skippy.hum',\n",
       " 'skippy.txt',\n",
       " 'slogans.txt',\n",
       " 'smackjok.hum',\n",
       " 'smartass.txt',\n",
       " 'smiley.txt',\n",
       " 'smokers.txt',\n",
       " 'smurf-03.txt',\n",
       " 'smurfkil.hum',\n",
       " 'smurfs.cc',\n",
       " 'smurf_co.txt',\n",
       " 'snapple.rum',\n",
       " 'snipe.txt',\n",
       " 'soccer.txt',\n",
       " 'socecon.hum',\n",
       " 'social.hum',\n",
       " 'socks.drx',\n",
       " 'solders.hum',\n",
       " 'soleleer.hum',\n",
       " 'solviets.hum',\n",
       " 'some_hu.mor',\n",
       " 'soporifi.abs',\n",
       " 'sorority.gir',\n",
       " 'spacever.hum',\n",
       " 'speling.msk',\n",
       " 'spelin_r.ifo',\n",
       " 'spider.hum',\n",
       " 'spoonlis.txt',\n",
       " 'spydust.hum',\n",
       " 'squids.gph',\n",
       " 'staff.txt',\n",
       " 'stagline.txt',\n",
       " 'standard.hum',\n",
       " 'startrek.txt',\n",
       " 'stereo.txt',\n",
       " 'steroid.txt',\n",
       " 'stone.hum',\n",
       " 'strattma.txt',\n",
       " 'stressman.txt',\n",
       " 'strine.txt',\n",
       " 'strsdiet.txt',\n",
       " 'studentb.txt',\n",
       " 'stuf10.txt',\n",
       " 'stuf11.txt',\n",
       " 'st_silic.txt',\n",
       " 'subb_lis.txt',\n",
       " 'subrdead.hum',\n",
       " 'suicide2.txt',\n",
       " 'sungenu.hum',\n",
       " 'supermar.rul',\n",
       " 'swearfrn.hum',\n",
       " 'sw_err.txt',\n",
       " 'symbol.hum',\n",
       " 'sysadmin.txt',\n",
       " 'sysman.txt',\n",
       " 't-10.hum',\n",
       " 't-shirt.hum',\n",
       " 'takenote.jok',\n",
       " 'talebeat.hum',\n",
       " 'talkbizr.txt',\n",
       " 'taping.hum',\n",
       " 'tarot.txt',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0c3f5",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b71a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    #Convert text to lowercase\n",
    "    lowercase_data = data.lower()\n",
    "    #print(\"=============Lowercase data==============\")\n",
    "    #print(lowercase_data)\n",
    "    \n",
    "    #  Removing Punctuation Marks and Word Tokenization\n",
    "#     tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "#     token_list = tokenizer.tokenize(lowercase_data)\n",
    "    token_list = nltk.word_tokenize(lowercase_data)\n",
    " \n",
    "    #print(\"=============Tokens==============\")\n",
    "    #print(token_list)\n",
    "\n",
    "    \n",
    "    #removing stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = []\n",
    "    for w in token_list:\n",
    "        if w not in stopwords:\n",
    "            tokens.append(w)\n",
    "    \n",
    "    #print(\"=============Tokens with stopwords removed==============\")\n",
    "    #print(tokens)\n",
    "    \n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in tokens]\n",
    "    #token_list = [word for word in tokens if word.isalnum()]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(\"=============Tokens with punctuation removed==============\")\n",
    "    #print(token_list)\n",
    "    \n",
    "    #Removing blank tokens\n",
    "    while(\"\" in token_list) :\n",
    "        token_list.remove(\"\")\n",
    "    \n",
    " \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b22977",
   "metadata": {},
   "source": [
    "# Finding Jaccard's Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b6b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findJaccardCoefficient(query, doc):\n",
    "    query_tokens = preprocessing(query)\n",
    "    doc_tokens = preprocessing(doc)\n",
    "    jaccard_coeff = len(set(query)&set(doc))/len(set(query)|set(doc))\n",
    "    return jaccard_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a16cf0",
   "metadata": {},
   "source": [
    "### Fetching top five documents as per the Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf6a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query : good day\n",
      "boston.geog\n",
      "johann\n",
      "justify\n",
      "bad.jok\n",
      "humpty.dumpty\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter the query : \")\n",
    "docsDict = {}\n",
    "jaccardCoeffOfDocs = []\n",
    "i=0\n",
    "# Find Jaccard Coefficient for each document and put it in in a list\n",
    "for file in docs:\n",
    "    docsDict[i]=file\n",
    "    filepath = directory+\"/\"+file\n",
    "    doc = open(filepath, 'r', encoding =\"ascii\", errors =\"surrogateescape\")\n",
    "    data = doc.read()\n",
    "    jaccardCoeff = findJaccardCoefficient(query,data)\n",
    "    jaccardCoeffOfDocs.append(jaccardCoeff)\n",
    "#     print(file,\"\\t\\t\", jaccardCoeff*100)\n",
    "    i+=1\n",
    "jaccardCoeffOfDocs = np.array(jaccardCoeffOfDocs)\n",
    "#Fetching the top 5 documents according to jaccard coefficient\n",
    "topFiveDocs = jaccardCoeffOfDocs.argsort()[-5:][::-1]\n",
    "for i in range(len(topFiveDocs)):\n",
    "    print(docsDict[topFiveDocs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba332b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c84bf0",
   "metadata": {},
   "source": [
    "# Finding TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "572e0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6f3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docCount = 0\n",
    "# frequency = pd.DataFrame()\n",
    "fileIdName = []\n",
    "fileTokens= []\n",
    "\n",
    "for file in docs:\n",
    "    filepath = directory+\"/\"+file\n",
    "    fileIdName.append([file])\n",
    "    file = open(filepath, 'r',encoding =\"ascii\", errors= 'surrogateescape')\n",
    "    data = \" \".join(file.read().split()) \n",
    "    file.close()\n",
    "    preprocessed_data = preprocessing(data)\n",
    "    fileTokens.append(preprocessed_data)\n",
    "fileIdName = pd.DataFrame(fileIdName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bae6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Raw Count-f(t,d)-----frequency of each term in doc\n",
    "\n",
    "RawFreqCount = []\n",
    "for itm in fileTokens:\n",
    "    raw_tf_dict ={}\n",
    "    for w in itm:\n",
    "        if raw_tf_dict.get(w) == None:  \n",
    "            raw_tf_dict[w] = 1\n",
    "        else:\n",
    "            raw_tf_dict[w] = raw_tf_dict.get(w)+1\n",
    "    RawFreqCount.append(raw_tf_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c1585c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-f(t,d)/∑f(t`,d)---frequncy of term divide by all no. of words in doc\n",
    "Term_Frequency = []     \n",
    "for itm in RawFreqCount:\n",
    "    tf_dict = {}\n",
    "    sum_tf = sum(itm.values())     \n",
    "    for w in itm.keys(): \n",
    "        tf_dict[w] = (itm.get(w)/sum_tf) \n",
    "    Term_Frequency.append(tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5440e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3- Log Normalisation = log(1 +f(t,d)))\n",
    "Log_Normalisation = []       \n",
    "for itm in RawFreqCount:\n",
    "    log_tf_dict = {}        \n",
    "    allkeys = itm.keys()\n",
    "    for w in allkeys:  \n",
    "        log_tf_dict[w] =  np.log(1+itm.get(w))     \n",
    "    Log_Normalisation.append(log_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71b9dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4---Double Normalisation Frequency = 0.5 + ((0.5*tf)/max(tf))\n",
    "Double_Normalisation = []  \n",
    "for itm in RawFreqCount:\n",
    "    dn_tf_dict = {}             \n",
    "    allkeys = itm.keys()\n",
    "    max_tf = max(itm.values())\n",
    "    for w in allkeys:  \n",
    "        dn_tf_dict[w] = 0.5 + (0.5*(itm.get(w)/max_tf)) \n",
    "    Double_Normalisation.append(dn_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f697b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5------Binary 0,1\n",
    "Binary_Term_Frequency = []  \n",
    "for itm in RawFreqCount:\n",
    "    bin_tf_dict = {}      \n",
    "    allkeys = itm.keys()\n",
    "    for w in allkeys:\n",
    "        freq = itm.get(w)\n",
    "        if freq >= 1 :          \n",
    "            bool_freq = 1 \n",
    "        else:\n",
    "            bool_freq = 0  \n",
    "        bin_tf_dict[w] = bool_freq\n",
    "    Binary_Term_Frequency.append(bin_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51b311ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency --no. of docs in which term t present\n",
    "docFreq = {}   \n",
    "for itm in fileTokens:\n",
    "    termsList = list(set(itm))\n",
    "    for wrd in termsList:\n",
    "        if docFreq.get(wrd) == None:\n",
    "            docFreq[wrd] = 1  \n",
    "        else: \n",
    "            docFreq[wrd] = docFreq.get(wrd)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dafe204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84064\n",
      "The Total Number of docs are :  1133\n"
     ]
    }
   ],
   "source": [
    "print(len(docFreq))\n",
    "N = len(docs)\n",
    "print('The Total Number of docs are : ',N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9114733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "keys_df = docFreq.keys()\n",
    "for item in keys_df:\n",
    "    idf[item] = np.log(N/docFreq[item] +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f29ef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function block for Calculating tf-idf value---tf*idf as tf=idf will be calculated based on all the 5 variants\n",
    "def get_tfidf(list_tf):\n",
    "    tfidf_list_resultant = []  \n",
    "    for itm in list_tf:\n",
    "        tfidf_dict = {}\n",
    "        allkeys = itm.keys()\n",
    "        for w in allkeys:  \n",
    "            tfidf = itm[w]  * idf[w]       \n",
    "            tfidf_dict[w] = tfidf\n",
    "        tfidf_list_resultant.append(tfidf_dict)\n",
    "    return tfidf_list_resultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22430195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating tfidf for all 5 variants\n",
    "binary_tfidf = get_tfidf(Binary_Term_Frequency)\n",
    "raw_count_tfidf = get_tfidf(RawFreqCount)\n",
    "term_frequency_tfidf = get_tfidf(Term_Frequency)\n",
    "log_normalisation_tfidf = get_tfidf(Log_Normalisation)\n",
    "double_normalisation_tfidf = get_tfidf(Double_Normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c18e71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84064"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_count_tfidf)\n",
    "all_terms = list(docFreq.keys())\n",
    "len(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "104e4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf score using log\n",
    "log_matrix_tfidf = []  \n",
    "for itm in log_normalisation_tfidf:\n",
    "    log_doc = []\n",
    "    for w in all_terms:\n",
    "        if w in itm :\n",
    "            log_doc.append(itm[w])  \n",
    "        else:\n",
    "            log_doc.append(0.0) \n",
    "    log_matrix_tfidf += [log_doc]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3a4cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf score using binary\n",
    "binary_matrix_tfidf = []  \n",
    "for itm in binary_tfidf:\n",
    "    bin_doc = []\n",
    "    for w in all_terms:\n",
    "        if w in itm :\n",
    "            bin_doc.append(itm[w]) \n",
    "        else:\n",
    "            bin_doc.append(0.0)   \n",
    "    binary_matrix_tfidf += [bin_doc]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "746e3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf score using double\n",
    "double_matrix_tfidf = []  \n",
    "for itm in double_normalisation_tfidf:\n",
    "    db_doc = []\n",
    "    for w in all_terms:\n",
    "        if w in itm :\n",
    "            temp = itm[w]\n",
    "            db_doc.append(temp) \n",
    "        else:\n",
    "            db_doc.append(0.0)  \n",
    "    double_matrix_tfidf += [db_doc]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4469a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##tf-idf score using term frequency\n",
    "term_matrix_tfidf = []   \n",
    "for itm in term_frequency_tfidf:\n",
    "    term_doc = []\n",
    "    for w in all_terms:\n",
    "        if w in itm :\n",
    "            term_doc.append(itm[w])  \n",
    "        else:\n",
    "            term_doc.append(0.0)\n",
    "    term_matrix_tfidf += [term_doc]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d627b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf score using raw count\n",
    "raw_matrix_tfidf = [] \n",
    "for itm in raw_count_tfidf:\n",
    "    inside_doc = []\n",
    "    for word in all_terms:\n",
    "        if word in itm :\n",
    "            inside_doc.append(itm[word]) \n",
    "        else:\n",
    "            inside_doc.append(0.0) \n",
    "    raw_matrix_tfidf += [inside_doc]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec6e9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching rank document based on tfidf\n",
    "def rank_document_tfidf(variant_list_tfidf, queries):\n",
    "    queryVector = [0.0] * len(all_terms)\n",
    "    t = set(queries)\n",
    "    res = [i for i, val in enumerate(all_terms) if val in t]\n",
    "\n",
    "    rank_doc = {}\n",
    "    for i in range(0,N):\n",
    "        rank_doc[i] = 0.0\n",
    "    for query_word in res:\n",
    "        count = -1\n",
    "        for itm in variant_list_tfidf:\n",
    "            count += 1\n",
    "            rank_doc[count] += itm[query_word]  \n",
    "    \n",
    "    for i in range(len(all_terms)):\n",
    "        if i in res:\n",
    "            queryVector[i]=1.0\n",
    "        else:\n",
    "            queryVector[i]=0.0\n",
    "\n",
    "\n",
    "    return dict(sorted(rank_doc.items(),key=operator.itemgetter(1),reverse=True)),queryVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dbe7753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_five_doc_tfidf(tfidf_list, queries, k, var):\n",
    "    #print(list_tfidf)\n",
    "    list_tf ,query_vector= rank_document_tfidf(tfidf_list,queries)\n",
    "    count = -1\n",
    "    flag = 0\n",
    "    list_final = []\n",
    "    for item in list_tf.keys():\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            print('Top ',k,' Documents based on ',var,' tf-idf are',list_final)\n",
    "            for i in list_final:\n",
    "                print(fileIdName.iloc[i][0])\n",
    "            flag = 1\n",
    "            break\n",
    "        list_final.append(item)\n",
    "    if flag == 0:\n",
    "        print('Top ',k,' Documents based on ',var,' tf-idf are:',list_final)    \n",
    "        for i in list_final:\n",
    "            print(fileIdName.iloc[i][0])\n",
    "    return query_vector         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10eca01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query token is calculated\n",
    "def query_tokens(query):\n",
    "    return word_tokenize(str(preprocessing(query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1cd370ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the String   : good day\n",
      "Top  5  Documents based on  Raw_Count  tf-idf are [519, 520, 977, 516, 663]\n",
      "insult.lst\n",
      "insults1.txt\n",
      "strine.txt\n",
      "insanity.hum\n",
      "mlverb.hum\n",
      "Top  5  Documents based on  Logarithmic  tf-idf are [519, 520, 222, 977, 516]\n",
      "insult.lst\n",
      "insults1.txt\n",
      "clancy.txt\n",
      "strine.txt\n",
      "insanity.hum\n",
      "Top  5  Documents based on  termfrequency  tf-idf are [963, 516, 1102, 520, 239]\n",
      "spelin_r.ifo\n",
      "insanity.hum\n",
      "wood\n",
      "insults1.txt\n",
      "cold.fus\n",
      "Top  5  Documents based on  Binary  tf-idf are [222, 1, 2, 6, 7]\n",
      "clancy.txt\n",
      "a-team\n",
      "abbott.txt\n",
      "acne1.txt\n",
      "acronym.lis\n",
      "Top  5  Documents based on  Double  tf-idf are [222, 84, 181, 239, 242]\n",
      "clancy.txt\n",
      "basehead.txt\n",
      "calif.hum\n",
      "cold.fus\n",
      "college.hum\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter the String   : ')\n",
    "query_afterPreprocess =query_tokens(query) \n",
    "query_vector_raw=top_five_doc_tfidf(raw_matrix_tfidf,query_afterPreprocess,5,'Raw_Count')\n",
    "query_vector_log=top_five_doc_tfidf(log_matrix_tfidf,query_afterPreprocess,5,'Logarithmic')   \n",
    "query_vector_term=top_five_doc_tfidf(term_matrix_tfidf,query_afterPreprocess,5,'termfrequency')\n",
    "query_vector_binary=top_five_doc_tfidf(binary_matrix_tfidf,query_afterPreprocess,5,'Binary')\n",
    "query_vector_double=top_five_doc_tfidf(double_matrix_tfidf,query_afterPreprocess,5,'Double') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9368bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe753b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
